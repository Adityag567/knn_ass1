{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217901f2-1da3-4b27-ab13-fc635a9eedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d7a28-1f5f-4333-83a8-9bae2e3d1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple and intuitive supervised machine learning algorithm used for classification\n",
    "and regression tasks. It works based on the principle that objects near each other in feature space share similar characteristics.\n",
    "The choice of 'K' (the number of neighbors to consider) is a crucial hyperparameter in the KNN algorithm. A larger K value makes the decision \n",
    "boundary smoother, but it may also lead to misclassification due to the inclusion of irrelevant neighbors. Conversely, \n",
    "a smaller K value makes the decision boundary more sensitive to noise in the dat\n",
    "KNN is a non-parametric, instance-based, and lazy learning algorithm, meaning it doesn't make any assumptions about the underlying data \n",
    "distribution and doesn't build an explicit model during training. Instead, it waits until a prediction request to do the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259fa0c4-bd8a-45b3-a950-5716f90d7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab1012-4b65-44e6-b028-152a1c951d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are some common methods for selecting an appropriate value of K:\n",
    "\n",
    "1.Grid Search: One approach is to perform a grid search over a range of K values, typically from 1 to a maximum value \n",
    "determined by the size of the training dataset. For each K value, the model's\n",
    "performance is evaluated using cross-validation, and the K value that results in the best performance metric (such as accuracy, \n",
    "F1-score, or mean squared error) is chosen.\n",
    "\n",
    "2.Odd Values: When dealing with binary classification tasks, it's often recommended to choose an odd value for K to avoid ties in the majority \n",
    "voting process. This helps in breaking ties when determining the class label for a new data point.\n",
    "\n",
    "3.Domain Knowledge: Understanding the problem domain and considering the characteristics of the dataset can provide insights into selecting \n",
    "an appropriate K value. For example, if the dataset has noisy features or outliers, a smaller K value may be preferable to avoid overfitting to the noise.\n",
    "\n",
    "4.Cross-Validation: Using techniques like k-fold cross-validation can help in assessing the robustness of the chosen K value. \n",
    "By splitting the dataset into multiple folds and averaging the performance across different splits, one can get a more reliable estimate of the model's generalization performance for various K values.\n",
    "\n",
    "Plotting Validation Curve: Plotting a validation curve showing the performance metric (e.g., accuracy) against different values of K can provide visual insights into how the model's performance changes with K. This can help in identifying the range of K values that yield optimal performance.\n",
    "\n",
    "Rule of Thumb: In practice, a commonly used rule of thumb is to choose K as the square root of the number of data points in the training set. However, this is just a starting point and may need adjustment based on the specific characteristics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201ba08-ee97-4bd9-a6ce-ac0f18f09dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b66957-5943-4ff4-8206-aaf3917fbf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task: The KNN classifier is used for classification tasks, where the goal is to predict the class label of a new data point based on its similarity to the labeled data points in the training set.\n",
    "Output: The output of a KNN classifier is a class label. When making predictions for a new data point, the classifier assigns it to the class that is most prevalent among its K-nearest \n",
    "\n",
    "Task: The KNN regressor, on the other hand, is used for regression tasks, where the goal is to predict a continuous numerical value for a new data point based on the values of its nearest neighbors in the training set.\n",
    "Output: The output of a KNN regressor is a numerical value. When making predictions for a new data point, the regressor computes the average (or weighted average) of the target values of its K-nearest neighbors and assigns it as the predicted value for the new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c21b49-368a-4a4b-b5dc-dd85137aca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b16dc8-7ced-4316-8593-239ecdc61d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "For Classification tasks:\n",
    "\n",
    "Accuracy: This is the most straightforward metric, measuring the proportion of correctly classified instances out of the total instances. It's calculated as the ratio of correct predictions to the total number of predictions.\n",
    "\n",
    "Precision: Precision measures the accuracy of positive predictions. It is the ratio of true positives to the sum of true positives and false positives. It focuses on the accuracy of positive predictions.\n",
    "\n",
    "Recall (Sensitivity): Recall measures the ability of the classifier to correctly identify positive instances. It is the ratio of true positives to the sum of true positives and false negatives. It focuses on the proportion of actual positives that were correctly identified.\n",
    "\n",
    "F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall, especially when the classes are imbalanced.\n",
    "\n",
    "ROC Curve and AUC: For binary classification tasks, the Receiver Operating Characteristic (ROC) curve plots the true positive rate against the false positive rate at various threshold settings. The Area Under the ROC Curve (AUC) summarizes the performance of the classifier across all possible thresholds, providing a single scalar value representing the model's ability to distinguish between classes.\n",
    "\n",
    "For Regression tasks:\n",
    "\n",
    "1.Mean Absolute Error (MAE): MAE measures the average absolute difference between the predicted and actual values. It provides a straightforward interpretation of the average prediction error.\n",
    "\n",
    "2.Mean Squared Error (MSE): MSE measures the average squared difference between the predicted and actual values. It penalizes larger errors more heavily than smaller errors.\n",
    "\n",
    "3.Root Mean Squared Error (RMSE): RMSE is the square root of the MSE, providing a measure of the average magnitude of error with the same units as the target variable.\n",
    "\n",
    "4.R-squared (R2): R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where 1 indicates a perfect fit and 0 indicates that the model does not explain any of the variability in the target variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147204c-89ec-4c3d-8d4b-ee84c7d22b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a3f13d-1aae-48c4-879d-e900e64b1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "The curse of dimensionality refers to the phenomenon where the performance of certain algorithms, including K-Nearest Neighbors (KNN),\n",
    "deteriorates as the number of features or dimensions in the dataset increases. In the context of KNN, the curse of dimensionality manifests in several ways:\n",
    "\n",
    "1.Increased Sparsity: As the number of dimensions increases, the volume of the feature space grows exponentially. Consequently,\n",
    "the available data becomes sparser, meaning that the density of data points in the feature space decreases. \n",
    "This sparsity can lead to difficulties in accurately estimating distances and identifying nearest neighbors, as there may not be enough data points nearby to provide meaningful comparisons.\n",
    "\n",
    "2.Increased Computational Complexity: Computing distances between data points becomes computationally expensive in high-dimensional spaces. The computational cost of finding nearest neighbors grows rapidly with the dimensionality of the data, making KNN slower and less efficient as the number of features increases.\n",
    "\n",
    "Overfitting: In high-dimensional spaces, there is a higher likelihood of overfitting due to the abundance of irrelevant or redundant features. The model may capture noise or idiosyncrasies specific to the training data, resulting in poor generalization to unseen data.\n",
    "\n",
    "Uniformity of Distances: In high-dimensional spaces, distances between data points tend to become more uniform or equivalent. This phenomenon implies that all data points are roughly equidistant from each other, diminishing the discriminatory power of distance-based algorithms like KNN.\n",
    "\n",
    "To mitigate the curse of dimensionality in KNN and other algorithms, several strategies can be employed:\n",
    "\n",
    "Feature Selection/Dimensionality Reduction: Choose a subset of relevant features or employ dimensionality reduction techniques (e.g., Principal Component Analysis) to reduce the number of dimensions while preserving as much information as possible.\n",
    "\n",
    "Distance Metrics: Utilize appropriate distance metrics (e.g., Mahalanobis distance) that account for the characteristics of high-dimensional data and alleviate the effects of sparsity.\n",
    "\n",
    "Data Preprocessing: Standardize or normalize the data to ensure that features are on a similar scale, which can help mitigate the impact of varying feature magnitudes on distance calculations.\n",
    "\n",
    "Regularization: Incorporate regularization techniques to prevent overfitting by penalizing overly complex models and discouraging the inclusion of irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff0761-2ed8-4ec0-843f-44360c2c3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0846d4-366f-4884-a673-5d618b9c4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Remove Instances: If few instances have missing values, delete them. But this can lose information.\n",
    "\n",
    "Imputation: Fill missing values with estimates:\n",
    "\n",
    "Mean/Median/Mode: Replace missing with average, median, or most frequent value.\n",
    "KNN Imputation: Predict missing values based on nearest neighbors.\n",
    "Regression: Use regression to predict missing values.\n",
    "Random: Fill missing with random values.\n",
    "Flagging: Create a flag for missing values instead of imputing them.\n",
    "\n",
    "Use Distance Metrics: Some metrics handle missing values by ignoring them during calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860a12f-0dca-4e8a-ae19-caeb5d1da5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e708f0-a1c5-4ff2-9aeb-85e5e1e77293",
   "metadata": {},
   "outputs": [],
   "source": [
    ", the K-Nearest Neighbors (KNN) classifier is suitable for classification tasks, providing discrete class labels based on similarities to existing labeled data.\n",
    "Its effective for non-linear decision boundaries, but sensitive to irrelevant features and computationally expensive for large datasets.\n",
    "\n",
    "On the other hand, the KNN regressor is ideal for regression tasks, predicting continuous numerical values based on the nearest neighbors' \n",
    "values. It's advantageous for capturing complex relationships and handling noisy data.\n",
    "\n",
    "The choice between KNN classifier and regressor depends on the problem type: use the classifier for categorical outcomes like spam detection, \n",
    "and the regressor for continuous predictions such as house prices. Each has its strengths and weaknesses, so selecting the appropriate \n",
    "one depends on the specific requirements and characteristics of the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fcdce0-fb10-46e4-95c8-ea1ea430c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6086e6e-392a-4f9e-a244-b6535cd83009",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntuitive and easy to understand.\n",
    "Non-parametric, making it versatile for various data distributions.\n",
    "No explicit training phase required.\n",
    "Effective for capturing non-linear relationships.\n",
    "Weaknesses:\n",
    "\n",
    "Computationally expensive with large datasets.\n",
    "Sensitive to noise and outliers.\n",
    "Affected by the curse of dimensionality in high-dimensional spaces.\n",
    "Requires careful selection of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097cfac-2162-4368-9a38-d189198c1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188f7c4-59c4-44fb-938e-63740597f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Euclidean Distance:\n",
    "\n",
    "Also known as straight-line distance or L2 norm.\n",
    "Measures the shortest straight path between two points in Euclidean space.\n",
    "Computed as the square root of the sum of the squares of the differences between corresponding coordinates.\n",
    "Gives more weight to differences in larger dimensions.\n",
    "Suitable for problems where the relationships between features are not linear.\n",
    "Manhattan Distance:\n",
    "\n",
    "Also known as city block distance or L1 norm.\n",
    "Measures the distance between two points by summing the absolute differences between their coordinates.\n",
    "Represents the distance traveled along axis-aligned paths (like navigating city blocks).\n",
    "Treats all dimensions equally and does not prioritize larger differences.\n",
    "Suitable for problems where the relationships between features are linear or when the feature space is sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86646b0a-d3ed-4514-b4c8-58ae21d76601",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9ec80-a042-40c6-8501-f6499ee33c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Equal Contribution: Feature scaling ensures that all features contribute equally to the distance calculations. Without scaling, features with larger scales may disproportionately influence the distance metric, leading to biased results.\n",
    "\n",
    "2.Improved Performance: Scaling can improve the performance and convergence of the KNN algorithm by making the distance calculations more consistent and preventing features with larger scales from dominating the distance computations.\n",
    "\n",
    "3.Distance Metric: Scaling ensures that the distance metric used in KNN, such as Euclidean distance or Manhattan distance, is meaningful and reflects the true dissimilarity between data points across all features.\n",
    "\n",
    "4.Consistency: Scaling makes the algorithm more consistent across different datasets and prevents the model from being overly sensitive to the scales of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe12e64-7864-4b5f-a1dc-3686ae32609f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
